{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# # Supervised Learning (Regression)\n",
    "# \n",
    "# In supervised learning, the task is to infer hidden structure from\n",
    "# labeled data, comprised of training examples $\\{(x_n, y_n)\\}$.\n",
    "# Regression typically means the output $y$ takes continuous values.\n",
    "# \n",
    "# We demonstrate with an example in Edward. A webpage version is available at\n",
    "# http://edwardlib.org/tutorials/supervised-regression.\n",
    "\n",
    "# In[1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: edward in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (1.3.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from edward) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from edward) (1.14.5)\n",
      "\u001b[31mkeras-preprocessing 1.0.1 has requirement keras>=2.1.6, but you'll have keras 2.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mkeras-applications 1.0.2 has requirement keras>=2.1.6, but you'll have keras 2.0.0 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install edward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().magic('matplotlib inline')\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import Normal\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Data\n",
    "# \n",
    "# Simulate training and test sets of $40$ data points. They comprise of\n",
    "# pairs of inputs $\\mathbf{x}_n\\in\\mathbb{R}^{10}$ and outputs\n",
    "# $y_n\\in\\mathbb{R}$. They have a linear dependence with normally\n",
    "# distributed noise.\n",
    "\n",
    "# In[2]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_toy_dataset(N, w):\n",
    "  D = len(w)\n",
    "  x = np.random.normal(0.0, 2.0, size=(N, D))\n",
    "  y = np.dot(x, w) + np.random.normal(0.0, 0.01, size=N)\n",
    "  return x, y\n",
    "\n",
    "\n",
    "ed.set_seed(42)\n",
    "\n",
    "N = 40  # number of data points\n",
    "D = 10  # number of features\n",
    "\n",
    "w_true = np.random.randn(D) * 0.5\n",
    "X_train, y_train = build_toy_dataset(N, w_true)\n",
    "X_test, y_test = build_toy_dataset(N, w_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Model\n",
    "# \n",
    "# Posit the model as Bayesian linear regression (Murphy, 2012).\n",
    "# It assumes a linear relationship between the inputs\n",
    "# $\\mathbf{x}\\in\\mathbb{R}^D$ and the outputs $y\\in\\mathbb{R}$.\n",
    "# \n",
    "# For a set of $N$ data points $(\\mathbf{X},\\mathbf{y})=\\{(\\mathbf{x}_n, y_n)\\}$,\n",
    "# the model posits the following distributions:\n",
    "# \n",
    "# \\begin{align*}\n",
    "#   p(\\mathbf{w})\n",
    "#   &=\n",
    "#   \\text{Normal}(\\mathbf{w} \\mid \\mathbf{0}, \\sigma_w^2\\mathbf{I}),\n",
    "#   \\\\[1.5ex]\n",
    "#   p(b)\n",
    "#   &=\n",
    "#   \\text{Normal}(b \\mid 0, \\sigma_b^2),\n",
    "#   \\\\\n",
    "#   p(\\mathbf{y} \\mid \\mathbf{w}, b, \\mathbf{X})\n",
    "#   &=\n",
    "#   \\prod_{n=1}^N\n",
    "#   \\text{Normal}(y_n \\mid \\mathbf{x}_n^\\top\\mathbf{w} + b, \\sigma_y^2).\n",
    "# \\end{align*}\n",
    "# \n",
    "# The latent variables are the linear model's weights $\\mathbf{w}$ and\n",
    "# intercept $b$, also known as the bias.\n",
    "# Assume $\\sigma_w^2,\\sigma_b^2$ are known prior variances and $\\sigma_y^2$ is a\n",
    "# known likelihood variance. The mean of the likelihood is given by a\n",
    "# linear transformation of the inputs $\\mathbf{x}_n$.\n",
    "# \n",
    "# Let's build the model in Edward, fixing $\\sigma_w,\\sigma_b,\\sigma_y=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'mu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3eda6a21c070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0med\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/edward/models/random_variable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mcollections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'collections'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mRANDOM_VARIABLE_COLLECTION\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_shape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_shape'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'mu'"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [N, D])\n",
    "w = Normal(mu=tf.zeros(D), sigma=tf.ones(D))\n",
    "b = Normal(mu=tf.zeros(1), sigma=tf.ones(1))\n",
    "y = Normal(mu=ed.dot(X, w) + b, sigma=tf.ones(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here, we define a placeholder `X`. During inference, we pass in\n",
    "# the value for this placeholder according to data.\n",
    "\n",
    "# ## Inference\n",
    "# \n",
    "# We now turn to inferring the posterior using variational inference.\n",
    "# Define the variational model to be a fully factorized normal across\n",
    "# the weights.\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "qw = Normal(mu=tf.Variable(tf.random_normal([D])),\n",
    "            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([D]))))\n",
    "qb = Normal(mu=tf.Variable(tf.random_normal([1])),\n",
    "            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([1]))))\n",
    "\n",
    "\n",
    "# Run variational inference with the Kullback-Leibler divergence, using \n",
    "# $250$ iterations and $5$ latent variable samples in the algorithm.\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "inference = ed.KLqp({w: qw, b: qb}, data={X: X_train, y: y_train})\n",
    "inference.run(n_samples=5, n_iter=250)\n",
    "\n",
    "\n",
    "# In this case `KLqp` defaults to minimizing the\n",
    "# $\\text{KL}(q\\|p)$ divergence measure using the reparameterization\n",
    "# gradient.\n",
    "# For more details on inference, see the [$\\text{KL}(q\\|p)$ tutorial](http://edwardlib.org/tutorials/klqp).\n",
    "\n",
    "# ## Criticism\n",
    "# \n",
    "# A standard evaluation for regression is to compare prediction accuracy on\n",
    "# held-out \"testing\" data. We do this by first forming the posterior predictive\n",
    "# distribution.\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "y_post = ed.copy(y, {w: qw, b: qb})\n",
    "# This is equivalent to\n",
    "# y_post = Normal(mu=ed.dot(X, qw) + qb, sigma=tf.ones(N))\n",
    "\n",
    "\n",
    "# With this we can evaluate various quantities using predictions from\n",
    "# the model (posterior predictive).\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "print(\"Mean squared error on test data:\")\n",
    "print(ed.evaluate('mean_squared_error', data={X: X_test, y_post: y_test}))\n",
    "\n",
    "print(\"Mean absolute error on test data:\")\n",
    "print(ed.evaluate('mean_absolute_error', data={X: X_test, y_post: y_test}))\n",
    "\n",
    "\n",
    "# The trained model makes predictions with low error\n",
    "# (relative to the magnitude of the output).\n",
    "# \n",
    "# We can also visualize the fit by comparing data generated with the\n",
    "# prior to data generated with the posterior (on the first feature\n",
    "# dimension).\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "def visualise(X_data, y_data, w, b, n_samples=10):\n",
    "  w_samples = w.sample(n_samples)[0].eval()\n",
    "  b_samples = b.sample(n_samples).eval()\n",
    "  plt.scatter(X_data[:, 0], y_data)\n",
    "  plt.ylim([-10, 10])\n",
    "  inputs = np.linspace(-8, 8, num=400)\n",
    "  for ns in range(n_samples):\n",
    "    output = inputs * w_samples[ns] + b_samples[ns]\n",
    "    plt.plot(inputs, output)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "# Visualize samples from the prior.\n",
    "visualise(X_train, y_train, w, b)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "# Visualize samples from the posterior.\n",
    "visualise(X_train, y_train, qw, qb)\n",
    "\n",
    "\n",
    "# The model has learned a linear relationship between the\n",
    "# first dimension of $\\mathbf{x}\\in\\mathbb{R}^D$ and the outputs\n",
    "# $y\\in\\mathbb{R}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
